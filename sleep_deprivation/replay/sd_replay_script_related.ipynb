{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from neuropy import plotting\n",
    "from tqdm.notebook import tqdm\n",
    "from neuropy.core import Epoch\n",
    "from neuropy.utils.mathutil import min_max_scaler\n",
    "from neuropy import plotting\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signficant sequences(wcorr) compare SD vs NSD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = subjects.pf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_df = []\n",
    "for sub, sess in enumerate(sessions):\n",
    "\n",
    "    maze = sess.paradigm[\"maze\"].flatten()\n",
    "    post = sess.paradigm[\"post\"].flatten()\n",
    "    zt_starts = np.array([0,2.5,5])\n",
    "    zt_stops = np.array([2.5,5,7.5])\n",
    "    epochs = np.vstack((zt_starts,zt_stops)).T* 3600 + post[0]\n",
    "    epochs = np.insert(epochs,0,maze,axis=0)\n",
    "    labels = ['MAZE','0-2.5','2.5-5','5-7.5']\n",
    "\n",
    "    for i, (t1,t2) in enumerate(epochs):\n",
    "        replay_pbe = sess.replay_pbe.time_slice(t1,t2).to_dataframe()\n",
    "        for direction in [\"up\", \"down\"]:\n",
    "            perc = replay_pbe[direction + \"_percentile_score\"].values\n",
    "            is_replay = (perc <= 5) | (perc >= 95)\n",
    "            scores = replay_pbe[direction + \"_score\"].values[is_replay]\n",
    "            order = replay_pbe[direction + \"_replay_order\"].values[is_replay]\n",
    "            order, counts = np.unique(order, return_counts=True)\n",
    "            df = pd.DataFrame(\n",
    "                dict(\n",
    "                    zt=labels[i],\n",
    "                    replay_order=[\"r\"],\n",
    "                    prop=counts.sum() / len(replay_pbe),\n",
    "                    # scores=np.abs(scores),\n",
    "                    animal=sub,\n",
    "                    grp=sess.tag,\n",
    "                )\n",
    "            )\n",
    "            replay_df.append(df)\n",
    "\n",
    "replay_df = pd.concat(replay_df, ignore_index=True)\n",
    "\n",
    "# subjects.GroupData().save(replay_df,'replay_sig_frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_,ax = plt.subplots() \n",
    "sns.barplot(data=replay_df, x=\"zt\", y=\"prop\",hue='grp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of wcorr SD vs NSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = subjects.pf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"#Sessions={len(sessions)}\")\n",
    "replay_df = []\n",
    "for sub, sess in enumerate(sessions):\n",
    "\n",
    "    pre = sess.paradigm[\"pre\"].flatten()\n",
    "    maze = sess.paradigm[\"maze\"].flatten()\n",
    "    post = sess.paradigm[\"post\"].flatten()\n",
    "\n",
    "    replay_pbe = pd.concat(\n",
    "        [sess.pre_replay_pbe.to_dataframe(), sess.replay_pbe.to_dataframe()],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    metadata = [sess.pre_replay_pbe.metadata, sess.replay_pbe.metadata]\n",
    "\n",
    "    wcorr_up = replay_pbe[\"up_score\"].values\n",
    "    up_shuffle = np.hstack([_[\"up_shuffle_score\"] for _ in metadata])\n",
    "\n",
    "    wcorr_down = -replay_pbe[\"down_score\"].values\n",
    "    down_shuffle = -np.hstack([_[\"down_shuffle_score\"] for _ in metadata])\n",
    "\n",
    "    best_bool = np.abs(wcorr_up) > np.abs(wcorr_down)\n",
    "\n",
    "    best_wcorr = np.zeros(len(wcorr_up))\n",
    "    best_wcorr[best_bool] = wcorr_up[best_bool]\n",
    "    best_wcorr[~best_bool] = wcorr_down[~best_bool]\n",
    "    best_shuffle = np.zeros_like(up_shuffle)\n",
    "    best_shuffle[:, best_bool] = up_shuffle[:, best_bool]\n",
    "    best_shuffle[:, ~best_bool] = down_shuffle[:, ~best_bool]\n",
    "\n",
    "    percentile = np.array(\n",
    "        [\n",
    "            stats.percentileofscore(best_shuffle[:, i], best_wcorr[i], kind=\"strict\")\n",
    "            for i in range(len(best_wcorr))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    abs_score = np.abs(best_wcorr)\n",
    "    abs_shuffle_score = np.abs(best_shuffle)\n",
    "    mean_shuffle_score = abs_shuffle_score.mean(axis=0)\n",
    "    std_shuffle_score = abs_shuffle_score.std(axis=0)\n",
    "    seq_score = (abs_score - mean_shuffle_score) / std_shuffle_score\n",
    "\n",
    "    zt_starts = np.array([0, 2.5, 5])\n",
    "    zt_stops = np.array([2.5, 5, 7.5])\n",
    "    epochs = np.vstack((zt_starts, zt_stops)).T * 3600 + post[0]\n",
    "    epochs = np.insert(epochs, 0, maze, axis=0)\n",
    "    epochs = np.insert(epochs, 0, pre, axis=0)\n",
    "    labels = [\"PRE\", \"MAZE\", \"0-2.5\", \"2.5-5\", \"5-7.5\"]\n",
    "\n",
    "    pval = np.zeros(len(best_wcorr))\n",
    "    positive_bool =best_wcorr <= 0 \n",
    "    pval[positive_bool] = percentile[positive_bool] / 1000\n",
    "    pval[~positive_bool] = 1 - percentile[~positive_bool] / 1000\n",
    "\n",
    "    for i, (t1, t2) in enumerate(epochs):\n",
    "        indx = (replay_pbe.start > t1) & (replay_pbe.start < t2)\n",
    "        df = pd.DataFrame(\n",
    "            dict(\n",
    "                zt=labels[i],\n",
    "                perc=percentile[indx],\n",
    "                seq_score=seq_score[indx],\n",
    "                pval=pval[indx],\n",
    "                wcorr=best_wcorr[indx],\n",
    "                animal=sub,\n",
    "                grp=sess.tag,\n",
    "            )\n",
    "        )\n",
    "        replay_df.append(df)\n",
    "\n",
    "replay_df = pd.concat(replay_df, ignore_index=True)\n",
    "\n",
    "# subjects.GroupData().save(replay_df, \"replay_wcorr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannotations.Annotator import Annotator\n",
    "from subjects import stat_kw\n",
    "\n",
    "_, axs = plt.subplots(2, 3)\n",
    "axs = axs.reshape(-1)\n",
    "colors = subjects.colors_sd(1)\n",
    "df = replay_df[replay_df.pval < 0.05]\n",
    "\n",
    "for i, t in enumerate(labels):\n",
    "    sns.histplot(\n",
    "        data=df[df.zt == t],\n",
    "        x=\"pval\",\n",
    "        hue=\"grp\",\n",
    "        common_bins=True,\n",
    "        common_norm=False,\n",
    "        stat=\"probability\",\n",
    "        ax=axs[i]\n",
    "    )\n",
    "# for g,grp in enumerate(['NSD','SD']):\n",
    "# df_new =df[df['grp']==grp]\n",
    "# plot_kw =dict(data=df,x='zt',y='pval',hue='grp',ax=ax,palette=subjects.colors_sd(1))\n",
    "# sns.violinplot(**plot_kw,showfliers=False,split=True,cut=True,scale='width')\n",
    "# ax.grid(axis='y',zorder=-1)\n",
    "\n",
    "# orders = replay_df.zt.unique()\n",
    "# pairs = [('PRE', _) for _ in orders[1:]]\n",
    "# annotator = Annotator(pairs=pairs, **plot_kw, order=orders)\n",
    "# annotator.configure(test=\"t-test_welch\", **stat_kw)\n",
    "# annotator.apply_and_annotate()\n",
    "# annotator.reset_configuration()\n",
    "\n",
    "\n",
    "# _,axs = plt.subplots(1,5)\n",
    "# for i,t in enumerate(labels):\n",
    "\n",
    "#     df =replay_df[replay_df.zt==t]\n",
    "#     d1 = df[df.grp=='NSD'].seq_score.values\n",
    "#     d2 = df[df.grp=='SD'].seq_score.values\n",
    "#     sns.ecdfplot(data=df, x=\"perc\",hue='grp',ax=axs[i],lw=1)\n",
    "#     print(stats.kstest(d1,d2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wcorr vs jump distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = subjects.pf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_jump_dist = lambda arr: np.diff(np.argmax(arr, axis=0)).mean()\n",
    "wcorr_jump_df = []\n",
    "for sub, sess in enumerate(sessions):\n",
    "\n",
    "    maze = sess.paradigm[\"maze\"].flatten()\n",
    "    post = sess.paradigm[\"post\"].flatten()\n",
    "    zt_starts = np.array([0, 2.5, 5])\n",
    "    zt_stops = np.array([2.5, 5, 7.5])\n",
    "    epochs = np.vstack((zt_starts, zt_stops)).T * 3600 + post[0]\n",
    "    epochs = np.insert(epochs, 0, maze, axis=0)\n",
    "    labels = [\"MAZE\", \"0-2.5\", \"2.5-5\", \"5-7.5\"]\n",
    "\n",
    "    replay_pbe = sess.replay_pbe.to_dataframe()\n",
    "\n",
    "    for direction in [\"up\", \"down\"]:\n",
    "        wcorr = np.abs(replay_pbe[direction + \"_score\"].values)\n",
    "        posteriors = sess.replay_pbe.metadata[direction + \"_posterior\"]\n",
    "        dx = 1 / posteriors[0].shape[0]\n",
    "        jump_dist = np.array([get_jump_dist(_) for _ in posteriors]) * dx\n",
    "        for i, (t1, t2) in enumerate(epochs):\n",
    "            indx = (replay_pbe.start > t1) & (replay_pbe.start < t2)\n",
    "\n",
    "            df = pd.DataFrame(\n",
    "                dict(\n",
    "                    zt=labels[i],\n",
    "                    wcorr=wcorr[indx],\n",
    "                    jump=np.abs(jump_dist[indx]),\n",
    "                    animal=sub,\n",
    "                    grp=sess.tag,\n",
    "                )\n",
    "            )\n",
    "            wcorr_jump_df.append(df)\n",
    "\n",
    "wcorr_jump_df = pd.concat(wcorr_jump_df, ignore_index=True)\n",
    "\n",
    "# subjects.GroupData().save(replay_df,'replay_wcorr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots(1,1)\n",
    "sns.violinplot(data=wcorr_jump_df,x='zt',y='jump',hue='grp',ax=ax)\n",
    "# sns.displot(data=wcorr_jump_df,x='jump',y='wcorr',binwidth=(0.05,0.05),col='grp')\n",
    "\n",
    "# _, axs = plt.subplots(1, 4)\n",
    "# for i, t in enumerate(labels):\n",
    "\n",
    "#     df = wcorr_jump_df[wcorr_jump_df.zt == t]\n",
    "#     d1 = df[df.grp=='NSD'].jump.values\n",
    "#     d2 = df[df.grp=='SD'].jump.values\n",
    "#     sns.barplot(\n",
    "#         data=df,\n",
    "#         x='zt',\n",
    "#         y=\"jump\",\n",
    "#         hue=\"grp\",\n",
    "#         ax=axs[i],\n",
    "#         # lw=1,\n",
    "#         # common_norm=False,\n",
    "#         # common_bins=True,\n",
    "#         # stat=\"probability\",\n",
    "#         # fill=False,\n",
    "#         # element='poly',\n",
    "#         # cumulative=True,\n",
    "#     )\n",
    "#     print(stats.kstest(d1,d2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signficant sequences histogram radon transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = subjects.nsd.pf_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_df = []\n",
    "for sub, sess in enumerate(sessions):\n",
    "\n",
    "    post = sess.paradigm[\"post\"].flatten()\n",
    "    replay_pbe = sess.replay_pbe.to_dataframe()\n",
    "    is_replay = replay_pbe.is_replay\n",
    "    velocity_bool = np.abs(replay_pbe.velocity.values) > 100\n",
    "\n",
    "    sig_events = np.logical_and(is_replay, velocity_bool)\n",
    "    starts = replay_pbe.start.values[sig_events]\n",
    "    bins = np.arange(post[0], post[0] + 2 * 3600, 600)\n",
    "    hist_replay = np.histogram(starts, bins)[0]\n",
    "    replay_df.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"bins\": ((bins[:-1] - post[0]) / 3600).round(2),\n",
    "                \"hist\": hist_replay,\n",
    "                \"grp\": sess.tag,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "replay_df = pd.concat(replay_df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plotting.Fig(grid=(2, 2))\n",
    "ax = fig.subplot(fig.gs[0])\n",
    "sns.barplot(data=replay_df, x=\"bins\", y=\"hist\", hue=\"grp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hfuncs import plot_in_bokeh\n",
    "import bokeh.plotting as bplot\n",
    "\n",
    "sess = subjects.nsd.ratSday2[0]\n",
    "post = sess.paradigm[\"post\"].flatten()\n",
    "replay_pbe = sess.replay_pbe.to_dataframe()\n",
    "starts = sess.replay_pbe.starts\n",
    "wcorr, scores, posteriors = [], [], []\n",
    "for i, d in enumerate([\"up\", \"down\"]):\n",
    "    scores.append(replay_pbe[d + \"_percentile_score\"].values)\n",
    "    posteriors.extend(sess.replay_pbe.metadata[d + \"_posterior\"])\n",
    "    wcorr.append(replay_pbe[d + \"_score\"].values)\n",
    "\n",
    "wcorr = np.concatenate(wcorr)\n",
    "scores = np.concatenate(scores)\n",
    "starts = np.tile(starts, (2,)) - post[0]\n",
    "scores_color = np.where((scores >= 95) | (scores <= 5), scores, 50)\n",
    "\n",
    "n_epochs = 7000\n",
    "annotate_dict = dict(evt=np.arange(n_epochs), wcorr=wcorr[:n_epochs])\n",
    "bplot.output_notebook()\n",
    "# bplot.output_file(subjects.figpath_sd/'test_wake_decoding.html')\n",
    "p = plot_in_bokeh(\n",
    "    starts[:n_epochs]/3600,\n",
    "    scores[:n_epochs],\n",
    "    [posteriors[_] for _ in range(n_epochs)],\n",
    "    annotate=annotate_dict,\n",
    "    color_by=scores_color[:n_epochs],\n",
    "    palette=\"jet\",\n",
    "    size=8,\n",
    ")\n",
    "# # p.line(pos.time,pos.x/350 + 0.2,line_width=2,color='black')\n",
    "bplot.show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples for figure4 sd paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = subjects.nsd.ratSday2 + subjects.sd.ratSday3\n",
    "\n",
    "# events= [[1221,2836,5072,2381,5179],[1326,1372,1824,1292,1516]]\n",
    "events= [[316,164,1416,3310,4201,3642,6728,6868],[263,148,2078,1900,3859,3468,6861,5978]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_example_dict = {}\n",
    "for sub, sess in enumerate(sessions):\n",
    "    replay = sess.replay_pbe\n",
    "    replay_df = sess.replay_pbe.to_dataframe()\n",
    "    starts = sess.replay_pbe.starts\n",
    "    wcorr, scores, posteriors = [], [], []\n",
    "    for i, d in enumerate([\"up\", \"down\"]):\n",
    "        scores.append(replay_df[d + \"_percentile_score\"].values)\n",
    "        posteriors.extend(replay.metadata[d + \"_posterior\"])\n",
    "        wcorr.append(replay_df[d + \"_score\"].values)\n",
    "    wcorr = np.concatenate(wcorr)\n",
    "    replay_example_dict[sess.tag] = dict(\n",
    "        posteriors=[posteriors[p] for p in events[sub]], wcorr=wcorr[events[sub]]\n",
    "    )\n",
    "\n",
    "subjects.GroupData().save(replay_example_dict,'replay_examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean replay score across all events as a function of time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subjects\n",
    "from neuropy import plotting\n",
    "from neuropy.utils import signal_process\n",
    "from neuropy.analyses import Pf1D\n",
    "from neuropy.analyses import Decode1d\n",
    "\n",
    "sessions = subjects.nsd.ratUday2 + subjects.sd.ratUday4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "replay_df = []\n",
    "for sub, sess in enumerate(sessions):\n",
    "\n",
    "    post = sess.paradigm[\"post\"].flatten()\n",
    "    replay_pbe = sess.replay_pbe.to_dataframe()\n",
    "    starts = sess.replay_pbe.starts\n",
    "    replay_score = replay_pbe.score\n",
    "    bins = np.arange(post[0], post[0] + 2 * 3600, 600)\n",
    "\n",
    "    mean_score = stats.binned_statistic(starts, replay_score, bins=bins)[0]\n",
    "    replay_df.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"bins\": ((bins[:-1] - post[0]) / 3600).round(2),\n",
    "                \"hist\": mean_score,\n",
    "                \"grp\": sess.tag,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "replay_df = pd.concat(replay_df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy import plotting\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plotting.Fig(grid=(2, 2))\n",
    "ax = fig.subplot(fig.gs[0])\n",
    "sns.lineplot(data=replay_df, x=\"bins\", y=\"hist\", hue=\"grp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of replay scores and comparison with shuffled scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subjects\n",
    "from neuropy import plotting\n",
    "from neuropy.utils import signal_process\n",
    "from neuropy.analyses import Pf1D\n",
    "from neuropy.analyses import Decode1d\n",
    "\n",
    "sessions = subjects.nsd.ratUday2 + subjects.sd.ratUday4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "score_df = []\n",
    "for sub, sess in enumerate(sessions):\n",
    "\n",
    "    post = sess.paradigm[\"post\"].flatten()\n",
    "    replay_pbe = sess.replay_pbe.to_dataframe()\n",
    "\n",
    "    starts = sess.replay_pbe.starts\n",
    "    replay_score = replay_pbe.score\n",
    "\n",
    "    shuffle_scores = sess.replay_pbe.metadata[\"shuffle_score\"].flatten()\n",
    "    shuffle_starts = np.repeat(starts, 200)\n",
    "\n",
    "    bins = np.arange(2) * 3600 + post[0]\n",
    "    replay_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Zt\": np.digitize(starts, bins),\n",
    "            \"starts\": starts,\n",
    "            \"score\": replay_score,\n",
    "            \"grp\": sess.tag,\n",
    "        }\n",
    "    )\n",
    "    shuffle_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Zt\": np.digitize(shuffle_starts, bins),\n",
    "            \"starts\": shuffle_starts,\n",
    "            \"score\": shuffle_scores,\n",
    "            \"grp\": f\"{sess.tag}_shuffle\",\n",
    "        }\n",
    "    )\n",
    "    score_df.append(pd.concat([replay_df, shuffle_df]))\n",
    "\n",
    "score_df = pd.concat(score_df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.replay_pbe.metadata[\"shuffle_score\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy import plotting\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plotting.Fig(grid=(2, 2))\n",
    "ax = fig.subplot(fig.gs[0])\n",
    "sns.violinplot(data=score_df, x=\"Zt\", y=\"score\", hue=\"grp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior time lag comparison SD vs NSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = subjects.nsd.ratUday2 + subjects.sd.ratUday4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as sg\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "lag_time_all = []\n",
    "for sub,sess in enumerate(sessions):\n",
    "    post = sess.paradigm['post'].flatten()\n",
    "    starts = sess.replay_pbe.starts\n",
    "    # ind = (starts>=post[0]+2.5*3600) &(starts<=post[0]+5*3600)\n",
    "    ind = starts<post[0]\n",
    "\n",
    "    posteriors = sess.replay_pbe.metadata['up_posterior']\n",
    "    posteriors = [posteriors[_] for _ in np.argwhere(ind).squeeze()] \n",
    "    posteriors = np.hstack(posteriors)\n",
    "\n",
    "    npos,nt = posteriors.shape\n",
    "    rows,cols = np.tril_indices(npos,-1)\n",
    "    lags = sg.correlation_lags(nt,nt,mode='same')*0.02\n",
    "    idx = (lags>=-0.5) & (lags<=0.5)\n",
    "    lags = lags[idx]\n",
    "\n",
    "    lag_time=np.zeros((npos,npos))\n",
    "    for p1 in range(npos):\n",
    "        for p2 in range(npos):\n",
    "            xcorr = sg.correlate(posteriors[p1],posteriors[p2],mode='same')[idx]\n",
    "            lag_time[p1,p2] = lags[gaussian_filter1d(xcorr,sigma=0.06/0.02).argmax()]\n",
    "\n",
    "    lag_time_all.append(lag_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(2,2)\n",
    "\n",
    "for i,lag_mat in enumerate(lag_time_all):\n",
    "\n",
    "    ax = axs[0,i]\n",
    "    im = ax.pcolormesh(lag_mat,cmap='bwr',vmin=-0.4,vmax=0.4)\n",
    "    cb = plt.colorbar(im,ax=ax)\n",
    "\n",
    "    ax2 = axs[1,i]\n",
    "    im = ax2.pcolormesh(np.abs(lag_mat),cmap='jet',vmin=0,vmax=0.4)\n",
    "    cb = plt.colorbar(im,ax=ax2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which positions are being replayed the most in POST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = subjects.nsd.ratUday2 + subjects.sd.ratUday4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as sg\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "mean_posterior = []\n",
    "for sub,sess in enumerate(sessions):\n",
    "    post = sess.paradigm['post'].flatten()\n",
    "    starts = sess.replay_pbe.starts\n",
    "    ind = (starts>=post[0]+2.5*3600) &(starts<=post[0]+5*3600)\n",
    "\n",
    "    posteriors = sess.replay_pbe.metadata['up_posterior']\n",
    "    posteriors = [posteriors[_] for _ in np.argwhere(ind).squeeze()] \n",
    "    mean_posterior.append(np.hstack(posteriors).mean(axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,2,sharey=True)\n",
    "\n",
    "for i,p in enumerate(mean_posterior):\n",
    "    axs[i].plot(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward and reverse replay during POST SD vs NSD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = subjects.pf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replay_funcs import get_jump_distance\n",
    "replay_df = []\n",
    "jd_df = []\n",
    "for sub, sess in enumerate(sessions):\n",
    "    maze = sess.paradigm['maze'].flatten()\n",
    "    post = sess.paradigm['post'].flatten()\n",
    "    replay_pbe_df = sess.replay_pbe.to_dataframe()\n",
    "    metadata = sess.replay_pbe.metadata\n",
    "\n",
    "    wcorr_up = replay_pbe_df[\"up_score\"].values\n",
    "    seq_score_up = replay_pbe_df[\"up_sequence_score\"].values\n",
    "    percentile_up = replay_pbe_df[\"up_percentile_score\"].values\n",
    "    jd_up = get_jump_distance(metadata['up_posterior'],'max')\n",
    "\n",
    "\n",
    "    wcorr_down = -replay_pbe_df[\"down_score\"].values\n",
    "    seq_score_down = replay_pbe_df[\"down_sequence_score\"].values\n",
    "    percentile_down = 100-replay_pbe_df[\"down_percentile_score\"].values\n",
    "    jd_down = get_jump_distance(metadata['down_posterior'],'max')\n",
    "\n",
    "\n",
    "    best_bool = np.abs(wcorr_up) > np.abs(wcorr_down)\n",
    "\n",
    "    wcorr = np.zeros(len(replay_pbe_df))\n",
    "    wcorr[best_bool] = wcorr_up[best_bool]\n",
    "    wcorr[~best_bool] = wcorr_down[~best_bool]\n",
    "\n",
    "    seq_score = np.zeros(len(replay_pbe_df))\n",
    "    seq_score[best_bool] = seq_score_up[best_bool]\n",
    "    seq_score[~best_bool] = seq_score_down[~best_bool]\n",
    "\n",
    "    percentile = np.zeros(len(replay_pbe_df))\n",
    "    percentile[best_bool] = percentile_up[best_bool]\n",
    "    percentile[~best_bool] = percentile_down[~best_bool]\n",
    "\n",
    "    jd= np.zeros(len(replay_pbe_df))\n",
    "    jd[best_bool] = jd_up[best_bool]\n",
    "    jd[~best_bool] = jd_down[~best_bool]\n",
    "\n",
    "\n",
    "\n",
    "    is_forward = (wcorr > 0) & ((percentile < 2.5) | (percentile > 97.5))\n",
    "    is_reverse = (wcorr < 0) & ((percentile < 2.5) | (percentile > 97.5))\n",
    "\n",
    "    zt_starts = np.array([0, 2.5, 5])\n",
    "    zt_stops = np.array([2.5, 5, 7.5])\n",
    "    epochs = np.vstack((zt_starts, zt_stops)).T * 3600 + post[0]\n",
    "    epochs = np.insert(epochs, 0, maze, axis=0)\n",
    "    labels = [\"MAZE\", \"0-2.5\", \"2.5-5\", \"5-7.5\"]\n",
    "\n",
    "    for i, (t1, t2) in enumerate(epochs):\n",
    "        indx = (replay_pbe_df.start > t1) & (replay_pbe_df.start < t2)\n",
    "        n_pbes = np.count_nonzero(indx)\n",
    "        df = pd.DataFrame(\n",
    "            dict(\n",
    "                zt=labels[i],\n",
    "                replay_order=[\"forward\", \"reverse\"],\n",
    "                prop=[\n",
    "                    np.count_nonzero(is_forward[indx]) / n_pbes,\n",
    "                    np.count_nonzero(is_reverse[indx]) / n_pbes,\n",
    "                ],\n",
    "                jd=jd[indx].mean(),\n",
    "                animal=sub,\n",
    "                grp=sess.tag,\n",
    "            )\n",
    "        )\n",
    "        replay_df.append(df)\n",
    "\n",
    "replay_df = pd.concat(replay_df, ignore_index=True)\n",
    "\n",
    "# subjects.GroupData().save(replay_df, \"replay_order\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,ax = plt.subplots(1,1)\n",
    "# sns.barplot(data=replay_df,x='zt',y='perc',hue='grp',ax=ax, ci =99)\n",
    "\n",
    "_, axs = plt.subplots(1, 2,sharey=True)\n",
    "\n",
    "for i, odr in enumerate([\"forward\", \"reverse\"]):\n",
    "    ax = axs[i]\n",
    "    df = replay_df[replay_df[\"replay_order\"] == odr]\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        x=\"zt\",\n",
    "        y=\"prop\",\n",
    "        hue=\"grp\",\n",
    "        ax=ax,\n",
    "        # split=True,\n",
    "        showfliers=False,\n",
    "        # estimator=np.median,\n",
    "        palette=subjects.colors_sd(1.4),\n",
    "        dodge=True,\n",
    "        # join=False,\n",
    "        # width='area',\n",
    "        # stat=\"probability\",\n",
    "        # common_norm=False,\n",
    "        # cumulative=True,\n",
    "        # showfliers=False,\n",
    "    )\n",
    "    # print(stats.ttest_ind(d1, d2,alternative='greater'))\n",
    "    ax.set_title(odr)\n",
    "    ax.set_ylim([0,0.3])\n",
    "\n",
    "\n",
    "# ax = axs[2]\n",
    "# sns.boxplot(\n",
    "#     data=df,\n",
    "#     x=\"zt\",\n",
    "#     y=\"jd\",\n",
    "#     hue=\"grp\",\n",
    "#     ax=ax,\n",
    "#     # split=True,\n",
    "#     showfliers=False,\n",
    "#     # estimator=np.median,\n",
    "#     palette=subjects.colors_sd(1.4)\n",
    "#     # dodge=True,\n",
    "#     # join=False,\n",
    "#     # width='area',\n",
    "#     # stat=\"probability\",\n",
    "#     # common_norm=False,\n",
    "#     # cumulative=True,\n",
    "#     # showfliers=False,\n",
    "# )\n",
    "# # print(stats.ttest_ind(d1, d2,alternative='greater'))\n",
    "# ax.set_title(odr)\n",
    "# # ax.set_ylim([0,0.3])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cce1618081139d19eb1ee8d40815d94a2de4f62e1efb20d9406ddb60628c36ae"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('data_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
